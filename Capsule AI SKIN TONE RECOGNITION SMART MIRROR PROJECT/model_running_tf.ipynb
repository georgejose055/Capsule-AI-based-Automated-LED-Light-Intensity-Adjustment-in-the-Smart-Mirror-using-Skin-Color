{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eedd0a33-3b3e-4efc-b19b-9e0b026942b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\georg\\anaconda3\\envs\\tf15\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "1.26.4\n",
      "2.15.0\n",
      "0.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "import ml_dtypes\n",
    "print(numpy.__version__)    # should be 1.26.4\n",
    "print(tf.__version__)       # should be 2.15.0\n",
    "print(ml_dtypes.__version__)# should be 0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4ba8f14-7fd1-476a-b367-885119f64d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\georg\\anaconda3\\envs\\tf15\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "TFLITE_MODEL_PATH = 'trained_models/mobilenetv2_skin_tone.tflite'  # Update if your model is in a different path\n",
    "CLASS_NAMES = ['black', 'brown', 'white']\n",
    "\n",
    "# Load TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']  # e.g., [1, 3, 224, 224] or [1, 224, 224, 3]\n",
    "\n",
    "# Preprocessing function (channels-first: [1, 3, 224, 224])\n",
    "def preprocess(img):\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "    img = img.transpose(2, 0, 1)  # Convert HWC -> CHW\n",
    "    img = np.expand_dims(img, axis=0)  # [1, 3, 224, 224]\n",
    "    return img\n",
    "\n",
    "# Haar cascades\n",
    "frontal_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_profileface.xml\")\n",
    "\n",
    "def forehead_coords(face):\n",
    "    x, y, w, h = face\n",
    "    fw = int(w * 0.5)\n",
    "    fh = int(h * 0.20)\n",
    "    fx = x + (w - fw) // 2\n",
    "    fy = y + int(h * 0.10)\n",
    "    return fx, fy, fw, fh\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    elapsed = int(time.time() - start_time)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = frontal_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100,100))\n",
    "    detected_type = \"Frontal\"\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        faces = profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100,100))\n",
    "        detected_type = \"Profile\"\n",
    "    if len(faces) == 0:\n",
    "        flipped = cv2.flip(gray, 1)\n",
    "        faces_flip = profile_cascade.detectMultiScale(flipped, scaleFactor=1.1, minNeighbors=5, minSize=(100,100))\n",
    "        if len(faces_flip) > 0:\n",
    "            faces = []\n",
    "            for (x, y, w, h) in faces_flip:\n",
    "                x_new = gray.shape[1] - x - w\n",
    "                faces.append((x_new, y, w, h))\n",
    "            detected_type = \"Profile (Flipped)\"\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        cv2.putText(frame, \"No face detected\", (30,40), cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0,0,255), 2)\n",
    "    else:\n",
    "        faces = sorted(faces, key=lambda x: x[2]*x[3], reverse=True)\n",
    "        (x, y, w, h) = faces[0]\n",
    "        fx, fy, fw, fh = forehead_coords((x, y, w, h))\n",
    "        fore_img = frame[fy:fy+fh, fx:fx+fw]\n",
    "\n",
    "        if fore_img.shape[0] > 10 and fore_img.shape[1] > 10:\n",
    "            img_rgb = cv2.cvtColor(fore_img, cv2.COLOR_BGR2RGB)\n",
    "            input_tensor = preprocess(img_rgb).astype(np.float32)\n",
    "            interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
    "            interpreter.invoke()\n",
    "            output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "            pred = np.argmax(output_data)\n",
    "            conf = float(tf.nn.softmax(output_data)[0][pred])\n",
    "            label = CLASS_NAMES[pred]\n",
    "            display_text = f\"{label} ({conf*100:.1f}%)\"\n",
    "        else:\n",
    "            display_text = \"N/A\"\n",
    "\n",
    "        # Mask and annotation\n",
    "        overlay = frame.copy()\n",
    "        cv2.rectangle(overlay, (fx, fy), (fx+fw, fy+fh), (255, 220, 0), -1)\n",
    "        frame = cv2.addWeighted(overlay, 0.3, frame, 0.7, 0)\n",
    "        cv2.rectangle(frame, (fx, fy), (fx+fw, fy+fh), (255, 220, 0), 2)\n",
    "        cv2.putText(frame, display_text, (fx, fy-5), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 220, 0), 2)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (128, 255, 0), 1)\n",
    "        cv2.putText(frame, f\"{detected_type}\", (x, y-25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,128,255), 2)\n",
    "\n",
    "    cv2.putText(frame, f\"{elapsed} seconds\", (10, frame.shape[0]-20),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Skin Tone Analysis (forehead/side profile)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e1a751-ff04-45eb-a218-4722c85cde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "MODEL_DIR = \"trained_models\"\n",
    "CLASS_NAMES = ['black', 'brown', 'white']\n",
    "\n",
    "PYTORCH_MODEL_PATH = os.path.join(MODEL_DIR, \"mobilenetv2_skin_tone.pth\")\n",
    "ONNX_MODEL_PATH = os.path.join(MODEL_DIR, \"mobilenetv2_skin_tone.onnx\")\n",
    "TF_SAVED_MODEL_PATH = os.path.join(MODEL_DIR, \"tf_saved_model\")\n",
    "TFLITE_MODEL_PATH = os.path.join(MODEL_DIR, \"mobilenetv2_skin_tone.tflite\")\n",
    "\n",
    "selected_format = \"tflite\" # set to \"pytorch\", \"onnx\", \"tf\", or \"tflite\"\n",
    "\n",
    "# If using PyTorch, you need your model definition file:\n",
    "# from model_def import MobileNetV2  # example, replace with your definition\n",
    "\n",
    "if selected_format == \"pytorch\":\n",
    "    import torch\n",
    "    from torchvision import transforms\n",
    "    import model_def_file  # Replace with your model's definition file\n",
    "    model = model_def_file.MobileNetV2()  # Replace with actual class name\n",
    "    model.load_state_dict(torch.load(PYTORCH_MODEL_PATH, map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "    def preprocess(img):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        return transform(img).unsqueeze(0)\n",
    "    def predict(input_img):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_img)\n",
    "            conf = torch.nn.functional.softmax(logits, dim=1)\n",
    "            pred = conf.argmax().item()\n",
    "            return pred, conf[0][pred].item()\n",
    "\n",
    "elif selected_format == \"onnx\":\n",
    "    import onnxruntime as ort\n",
    "    ort_session = ort.InferenceSession(ONNX_MODEL_PATH)\n",
    "    def preprocess(img):\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "        img = img.transpose(2,0,1)[np.newaxis,:]\n",
    "        return img\n",
    "    def predict(input_img):\n",
    "        output = ort_session.run(None, {ort_session.get_inputs()[0].name: input_img})[0]\n",
    "        pred = np.argmax(output)\n",
    "        softmax = np.exp(output) / np.exp(output).sum()\n",
    "        conf = float(softmax[0][pred])\n",
    "        return pred, conf\n",
    "\n",
    "elif selected_format == \"tf\":\n",
    "    import tensorflow as tf\n",
    "    model = tf.saved_model.load(TF_SAVED_MODEL_PATH)\n",
    "    infer = model.signatures[\"serving_default\"] if \"serving_default\" in model.signatures else model\n",
    "    # Assumes input signature will work as below; adjust if needed.\n",
    "    def preprocess(img):\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "        img = img[np.newaxis, ...]\n",
    "        return img\n",
    "    def predict(input_img):\n",
    "        inputs = tf.convert_to_tensor(input_img)\n",
    "        outputs = infer(inputs)\n",
    "        logits = list(outputs.values())[0] if isinstance(outputs, dict) else outputs\n",
    "        conf = tf.nn.softmax(logits, axis=-1)\n",
    "        pred = tf.argmax(conf, axis=-1).numpy()[0]\n",
    "        return pred, float(conf[0][pred])\n",
    "\n",
    "else:  # TFLite\n",
    "    import tensorflow as tf\n",
    "    interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    def preprocess(img):\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "        img = img.transpose(2, 0, 1)  # CHW\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    def predict(input_img):\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_img.astype(np.float32))\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        pred = np.argmax(output_data)\n",
    "        softmax = np.exp(output_data) / np.exp(output_data).sum()\n",
    "        conf = float(softmax[0][pred])\n",
    "        return pred, conf\n",
    "\n",
    "# Haar cascades for face detection\n",
    "frontal_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_profileface.xml\")\n",
    "\n",
    "def forehead_coords(face):\n",
    "    x, y, w, h = face\n",
    "    fw = int(w * 0.5)\n",
    "    fh = int(h * 0.20)\n",
    "    fx = x + (w - fw) // 2\n",
    "    fy = y + int(h * 0.10)\n",
    "    return fx, fy, fw, fh\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    elapsed = int(time.time() - start_time)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = frontal_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "    detected_type = \"Frontal\"\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        faces = profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "        detected_type = \"Profile\"\n",
    "    if len(faces) == 0:\n",
    "        flipped = cv2.flip(gray, 1)\n",
    "        faces_flip = profile_cascade.detectMultiScale(flipped, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "        if len(faces_flip) > 0:\n",
    "            faces = []\n",
    "            for (x, y, w, h) in faces_flip:\n",
    "                x_new = gray.shape[1] - x - w\n",
    "                faces.append((x_new, y, w, h))\n",
    "            detected_type = \"Profile (Flipped)\"\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        cv2.putText(frame, \"No face detected\", (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0, 0, 255), 2)\n",
    "    else:\n",
    "        faces = sorted(faces, key=lambda x: x[2]*x[3], reverse=True)\n",
    "        (x, y, w, h) = faces[0]\n",
    "        fx, fy, fw, fh = forehead_coords((x, y, w, h))\n",
    "        fore_img = frame[fy:fy+fh, fx:fx+fw]\n",
    "\n",
    "        if fore_img.shape[0] > 10 and fore_img.shape[1] > 10:\n",
    "            img_rgb = cv2.cvtColor(fore_img, cv2.COLOR_BGR2RGB)\n",
    "            if selected_format == \"pytorch\":\n",
    "                input_tensor = preprocess(img_rgb)\n",
    "                pred, conf = predict(input_tensor)\n",
    "            elif selected_format == \"onnx\":\n",
    "                input_tensor = preprocess(img_rgb)\n",
    "                pred, conf = predict(input_tensor)\n",
    "            elif selected_format == \"tf\":\n",
    "                input_tensor = preprocess(img_rgb)\n",
    "                pred, conf = predict(input_tensor)\n",
    "            else:  # TFLite\n",
    "                input_tensor = preprocess(img_rgb)\n",
    "                pred, conf = predict(input_tensor)\n",
    "            label = CLASS_NAMES[pred]\n",
    "            display_text = f\"{label} ({conf*100:.1f}%)\"\n",
    "        else:\n",
    "            display_text = \"N/A\"\n",
    "\n",
    "        # Overlay and annotation\n",
    "        overlay = frame.copy()\n",
    "        cv2.rectangle(overlay, (fx, fy), (fx+fw, fy+fh), (255, 220, 0), -1)\n",
    "        frame = cv2.addWeighted(overlay, 0.3, frame, 0.7, 0)\n",
    "        cv2.rectangle(frame, (fx, fy), (fx+fw, fy+fh), (255, 220, 0), 2)\n",
    "        cv2.putText(frame, display_text, (fx, fy-5), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 220, 0), 2)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (128, 255, 0), 1)\n",
    "        cv2.putText(frame, f\"{detected_type}\", (x, y-25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,128,255), 2)\n",
    "\n",
    "    cv2.putText(frame, f\"{elapsed} seconds\", (10, frame.shape[0]-20),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Skin Tone Analysis (forehead/side profile)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb7e3cb-a09a-4946-9836-0b5c3a1f218b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model_def_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 102\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m preprocess, predict\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Initialize model functions\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m preprocess, predict \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Haar cascades for face detection\u001b[39;00m\n\u001b[0;32m    105\u001b[0m frontal_cascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(cv2\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mhaarcascades \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(selected_format)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel_def_file\u001b[39;00m  \u001b[38;5;66;03m# Replace with your PyTorch model definition file\u001b[39;00m\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m model_def_file\u001b[38;5;241m.\u001b[39mMobileNetV2()  \u001b[38;5;66;03m# Replace with your actual class\u001b[39;00m\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(PYTORCH_MODEL_PATH, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'model_def_file'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "MODEL_DIR = \"trained_models\"\n",
    "CLASS_NAMES = ['black', 'brown', 'white']\n",
    "\n",
    "PYTORCH_MODEL_PATH = os.path.join(MODEL_DIR, \"mobilenetv2_skin_tone.pth\")\n",
    "ONNX_MODEL_PATH = os.path.join(MODEL_DIR, \"mobilenetv2_skin_tone.onnx\")\n",
    "TF_SAVED_MODEL_PATH = os.path.join(MODEL_DIR, \"tf_saved_model\")\n",
    "TFLITE_MODEL_PATH = os.path.join(MODEL_DIR, \"mobilenetv2_skin_tone.tflite\")\n",
    "\n",
    "selected_format = \"tflite\"  # Change to \"pytorch\", \"onnx\", \"tf\", or \"tflite\"\n",
    "\n",
    "def load_model(selected_format):\n",
    "    if selected_format == \"pytorch\":\n",
    "        import torch\n",
    "        from torchvision import transforms\n",
    "        import model_def_file  # Replace with your PyTorch model definition file\n",
    "        model = model_def_file.MobileNetV2()  # Replace with your actual class\n",
    "        model.load_state_dict(torch.load(PYTORCH_MODEL_PATH, map_location=\"cpu\"))\n",
    "        model.eval()\n",
    "        def preprocess(img):\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "            return transform(img).unsqueeze(0)\n",
    "        def predict(input_img):\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_img)\n",
    "                conf = torch.nn.functional.softmax(logits, dim=1)\n",
    "                pred = conf.argmax().item()\n",
    "                return pred, conf[0][pred].item()\n",
    "        return preprocess, predict\n",
    "\n",
    "    elif selected_format == \"onnx\":\n",
    "        import onnxruntime as ort\n",
    "        ort_session = ort.InferenceSession(ONNX_MODEL_PATH)\n",
    "        def preprocess(img):\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "            img = img.transpose(2, 0, 1)[np.newaxis, ...]  # NCHW\n",
    "            return img\n",
    "        def predict(input_img):\n",
    "            output = ort_session.run(None, {ort_session.get_inputs()[0].name: input_img})[0]\n",
    "            pred = np.argmax(output)\n",
    "            softmax = np.exp(output) / np.exp(output).sum()\n",
    "            conf = float(softmax[0][pred])\n",
    "            return pred, conf\n",
    "        return preprocess, predict\n",
    "\n",
    "    elif selected_format == \"tf\":\n",
    "        import tensorflow as tf\n",
    "        model = tf.saved_model.load(TF_SAVED_MODEL_PATH)\n",
    "        infer = model.signatures[\"serving_default\"] if \"serving_default\" in model.signatures else model\n",
    "        def preprocess(img):\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "            img = img[np.newaxis, ...]  # NHWC\n",
    "            return img\n",
    "        def predict(input_img):\n",
    "            import tensorflow as tf\n",
    "            inputs = tf.convert_to_tensor(input_img)\n",
    "            outputs = infer(inputs)\n",
    "            logits = list(outputs.values())[0] if isinstance(outputs, dict) else outputs\n",
    "            conf = tf.nn.softmax(logits, axis=-1)\n",
    "            pred = tf.argmax(conf, axis=-1).numpy()[0]\n",
    "            return pred, float(conf[0][pred])\n",
    "        return preprocess, predict\n",
    "\n",
    "    else:  # tflite\n",
    "        import tensorflow as tf\n",
    "        interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "        interpreter.allocate_tensors()\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        def preprocess(img):\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "            img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "            # Adjust to model input: CHW or HWC (adjust if needed for your export!)\n",
    "            img = img.transpose(2, 0, 1)  # NCHW\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            return img\n",
    "        def predict(input_img):\n",
    "            interpreter.set_tensor(input_details[0]['index'], input_img.astype(np.float32))\n",
    "            interpreter.invoke()\n",
    "            output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "            pred = np.argmax(output_data)\n",
    "            softmax = np.exp(output_data) / np.exp(output_data).sum()\n",
    "            conf = float(softmax[0][pred])\n",
    "            return pred, conf\n",
    "        return preprocess, predict\n",
    "\n",
    "# Initialize model functions\n",
    "preprocess, predict = load_model(selected_format)\n",
    "\n",
    "# Haar cascades for face detection\n",
    "frontal_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_profileface.xml\")\n",
    "\n",
    "def forehead_coords(face):\n",
    "    x, y, w, h = face\n",
    "    fw = int(w * 0.5)\n",
    "    fh = int(h * 0.20)\n",
    "    fx = x + (w - fw) // 2\n",
    "    fy = y + int(h * 0.10)\n",
    "    return fx, fy, fw, fh\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    elapsed = int(time.time() - start_time)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = frontal_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "    detected_type = \"Frontal\"\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        faces = profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "        detected_type = \"Profile\"\n",
    "    if len(faces) == 0:\n",
    "        flipped = cv2.flip(gray, 1)\n",
    "        faces_flip = profile_cascade.detectMultiScale(flipped, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "        if len(faces_flip) > 0:\n",
    "            faces = []\n",
    "            for (x, y, w, h) in faces_flip:\n",
    "                x_new = gray.shape[1] - x - w\n",
    "                faces.append((x_new, y, w, h))\n",
    "            detected_type = \"Profile (Flipped)\"\n",
    "    if len(faces) == 0:\n",
    "        cv2.putText(frame, \"No face detected\", (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0, 0, 255), 2)\n",
    "    else:\n",
    "        faces = sorted(faces, key=lambda x: x[2] * x[3], reverse=True)\n",
    "        (x, y, w, h) = faces[0]\n",
    "        fx, fy, fw, fh = forehead_coords((x, y, w, h))\n",
    "        fore_img = frame[fy:fy+fh, fx:fx+fw]\n",
    "\n",
    "        if fore_img.shape[0] > 10 and fore_img.shape[1] > 10:\n",
    "            img_rgb = cv2.cvtColor(fore_img, cv2.COLOR_BGR2RGB)\n",
    "            input_tensor = preprocess(img_rgb)\n",
    "            pred, conf = predict(input_tensor)\n",
    "            label = CLASS_NAMES[pred]\n",
    "            display_text = f\"{label} ({conf * 100:.1f}%)\"\n",
    "        else:\n",
    "            display_text = \"N/A\"\n",
    "\n",
    "        # Overlay/annotation\n",
    "        overlay = frame.copy()\n",
    "        cv2.rectangle(overlay, (fx, fy), (fx+fw, fy+fh), (255, 220, 0), -1)\n",
    "        frame = cv2.addWeighted(overlay, 0.3, frame, 0.7, 0)\n",
    "        cv2.rectangle(frame, (fx, fy), (fx+fw, fy+fh), (255, 220, 0), 2)\n",
    "        cv2.putText(frame, display_text, (fx, fy-5), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 220, 0), 2)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (128, 255, 0), 1)\n",
    "        cv2.putText(frame, f\"{detected_type}\", (x, y-25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 128, 255), 2)\n",
    "\n",
    "    cv2.putText(frame, f\"{elapsed} seconds\", (10, frame.shape[0]-20),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Skin Tone Analysis (forehead/side profile)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836ae2d6-33c7-4da7-b3a4-8573ade810b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "MODEL_DIR = \"trained_models\"\n",
    "CLASS_NAMES = ['black', 'brown', 'white']\n",
    "PYTORCH_MODEL_PATH = os.path.join(MODEL_DIR, \"mobilenetv2_skin_tone.pth\")\n",
    "\n",
    "def load_model():\n",
    "    import torch\n",
    "    from torchvision.models import mobilenet_v2\n",
    "    from torchvision import transforms\n",
    "\n",
    "    model = mobilenet_v2(num_classes=3)\n",
    "    model.load_state_dict(torch.load(PYTORCH_MODEL_PATH, map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "    def preprocess(img):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        return transform(img).unsqueeze(0)\n",
    "    def predict(input_img):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_img)\n",
    "            conf = torch.nn.functional.softmax(logits, dim=1)\n",
    "            pred = conf.argmax().item()\n",
    "            return pred, conf[0][pred].item()\n",
    "    return preprocess, predict\n",
    "\n",
    "preprocess, predict = load_model()\n",
    "frontal_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_profileface.xml\")\n",
    "\n",
    "def forehead_coords(face):\n",
    "    x, y, w, h = face\n",
    "    fw = int(w * 0.5)\n",
    "    fh = int(h * 0.20)\n",
    "    fx = x + (w - fw) // 2\n",
    "    fy = y + int(h * 0.10)\n",
    "    return fx, fy, fw, fh\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    elapsed = int(time.time() - start_time)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = frontal_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "    detected_type = \"Frontal\"\n",
    "    if len(faces) == 0:\n",
    "        faces = profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "        detected_type = \"Profile\"\n",
    "    if len(faces) == 0:\n",
    "        flipped = cv2.flip(gray, 1)\n",
    "        faces_flip = profile_cascade.detectMultiScale(flipped, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "        if len(faces_flip) > 0:\n",
    "            faces = []\n",
    "            for (x, y, w, h) in faces_flip:\n",
    "                x_new = gray.shape[1] - x - w\n",
    "                faces.append((x_new, y, w, h))\n",
    "            detected_type = \"Profile (Flipped)\"\n",
    "    if len(faces) == 0:\n",
    "        cv2.putText(frame, \"No face detected\", (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0, 0, 255), 2)\n",
    "    else:\n",
    "        faces = sorted(faces, key=lambda x: x[2]*x[3], reverse=True)\n",
    "        (x, y, w, h) = faces[0]\n",
    "        fx, fy, fw, fh = forehead_coords((x, y, w, h))\n",
    "        fore_img = frame[fy:fy+fh, fx:fx+fw]\n",
    "        if fore_img.shape[0] > 10 and fore_img.shape[1] > 10:\n",
    "            img_rgb = cv2.cvtColor(fore_img, cv2.COLOR_BGR2RGB)\n",
    "            input_tensor = preprocess(img_rgb)\n",
    "            pred, conf = predict(input_tensor)\n",
    "            label = CLASS_NAMES[pred]\n",
    "            display_text = f\"{label} ({conf * 100:.1f}%)\"\n",
    "        else:\n",
    "            display_text = \"N/A\"\n",
    "        overlay = frame.copy()\n",
    "        cv2.rectangle(overlay, (fx, fy), (fx+fw, fy+fh), (255, 220, 0), -1)\n",
    "        frame = cv2.addWeighted(overlay, 0.3, frame, 0.7, 0)\n",
    "        cv2.rectangle(frame, (fx, fy), (fx+fw, fy+fh), (255, 220, 0), 2)\n",
    "        cv2.putText(frame, display_text, (fx, fy-5), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,220,0), 2)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (128,255,0), 1)\n",
    "        cv2.putText(frame, f\"{detected_type}\", (x, y-25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,128,255), 2)\n",
    "    cv2.putText(frame, f\"{elapsed} seconds\", (10, frame.shape[0]-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    cv2.imshow(\"Skin Tone Analysis (forehead/side profile)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6294b617-99d5-4de2-b4a4-15a77ffa5ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute __inference_signature_wrapper_2241 as input #0(zero-based) was expected to be a float tensor but is a double tensor [Op:__inference_signature_wrapper_2241]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\function_type_utils.py:442\u001b[0m, in \u001b[0;36mbind_function_inputs\u001b[1;34m(args, kwargs, function_type, default_values)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_values\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:264\u001b[0m, in \u001b[0;36mFunctionType.bind_with_defaults\u001b[1;34m(self, args, kwargs, default_values)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns BoundArguments with default values filled in.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    265\u001b[0m bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\inspect.py:3186\u001b[0m, in \u001b[0;36mSignature.bind\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[0;32m   3183\u001b[0m \u001b[38;5;124;03mand `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[0;32m   3184\u001b[0m \u001b[38;5;124;03mif the passed arguments can not be bound.\u001b[39;00m\n\u001b[0;32m   3185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\inspect.py:3112\u001b[0m, in \u001b[0;36mSignature._bind\u001b[1;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[0;32m   3109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m (_VAR_KEYWORD, _KEYWORD_ONLY):\n\u001b[0;32m   3110\u001b[0m     \u001b[38;5;66;03m# Looks like we have no parameter for this positional\u001b[39;00m\n\u001b[0;32m   3111\u001b[0m     \u001b[38;5;66;03m# argument\u001b[39;00m\n\u001b[1;32m-> 3112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   3113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo many positional arguments\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m _VAR_POSITIONAL:\n\u001b[0;32m   3116\u001b[0m     \u001b[38;5;66;03m# We have an '*args'-like argument, let's fill it with\u001b[39;00m\n\u001b[0;32m   3117\u001b[0m     \u001b[38;5;66;03m# all positional arguments we have left and move on to\u001b[39;00m\n\u001b[0;32m   3118\u001b[0m     \u001b[38;5;66;03m# the next phase\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: too many positional arguments",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1180\u001b[0m, in \u001b[0;36mConcreteFunction._call_impl\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1180\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_structured_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m structured_err:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1260\u001b[0m, in \u001b[0;36mConcreteFunction._call_with_structured_signature\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Executes the wrapped function with the structured signature.\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m \n\u001b[0;32m   1248\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;124;03m    of this `ConcreteFunction`.\u001b[39;00m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1260\u001b[0m     \u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonicalize_function_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1262\u001b[0m )\n\u001b[0;32m   1263\u001b[0m filtered_flat_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\function_type_utils.py:422\u001b[0m, in \u001b[0;36mcanonicalize_function_inputs\u001b[1;34m(args, kwargs, function_type, default_values, is_pure)\u001b[0m\n\u001b[0;32m    421\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m _convert_variables_to_tensors(args, kwargs)\n\u001b[1;32m--> 422\u001b[0m bound_arguments \u001b[38;5;241m=\u001b[39m \u001b[43mbind_function_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_values\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bound_arguments\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\function_type_utils.py:446\u001b[0m, in \u001b[0;36mbind_function_inputs\u001b[1;34m(args, kwargs, function_type, default_values)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 446\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    447\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBinding inputs to tf.function failed due to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for signature:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    450\u001b[0m   ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bound_arguments\n",
      "\u001b[1;31mTypeError\u001b[0m: Binding inputs to tf.function failed due to `too many positional arguments`. Received args: (<tf.Tensor: shape=(1, 224, 224, 3), dtype=float64, numpy=\narray([[[[-1.53566227, -1.44047616, -1.21185182],\n         [-1.53566227, -1.44047616, -1.21185182],\n         [-1.53566227, -1.44047616, -1.21185182],\n         ...,\n         [-0.69654932, -0.56512601, -0.39267969],\n         [-0.66229981, -0.530112  , -0.3578213 ],\n         [-0.64517506, -0.512605  , -0.34039211]],\n\n        [[-1.53566227, -1.44047616, -1.21185182],\n         [-1.53566227, -1.44047616, -1.21185182],\n         [-1.53566227, -1.44047616, -1.21185182],\n         ...,\n         [-0.69654932, -0.56512601, -0.39267969],\n         [-0.66229981, -0.530112  , -0.3578213 ],\n         [-0.64517506, -0.512605  , -0.34039211]],\n\n        [[-1.53566227, -1.44047616, -1.21185182],\n         [-1.53566227, -1.44047616, -1.21185182],\n         [-1.53566227, -1.44047616, -1.21185182],\n         ...,\n         [-0.67942456, -0.547619  , -0.3752505 ],\n         [-0.66229981, -0.530112  , -0.3578213 ],\n         [-0.64517506, -0.512605  , -0.34039211]],\n\n        ...,\n\n        [[-0.66229981, -0.86274503, -0.67154678],\n         [-0.67942456, -0.88025204, -0.68897597],\n         [-0.69654932, -0.89775904, -0.72383442],\n         ...,\n         [ 0.24531222,  0.22268921,  0.39163399],\n         [ 0.26243697,  0.24019621,  0.40906318],\n         [ 0.27956172,  0.25770321,  0.42649251]],\n\n        [[-0.66229981, -0.86274503, -0.67154678],\n         [-0.67942456, -0.88025204, -0.68897597],\n         [-0.69654932, -0.89775904, -0.72383442],\n         ...,\n         [ 0.24531222,  0.22268921,  0.39163399],\n         [ 0.26243697,  0.24019621,  0.40906318],\n         [ 0.27956172,  0.25770321,  0.42649251]],\n\n        [[-0.66229981, -0.86274503, -0.67154678],\n         [-0.67942456, -0.88025204, -0.68897597],\n         [-0.69654932, -0.89775904, -0.72383442],\n         ...,\n         [ 0.24531222,  0.22268921,  0.39163399],\n         [ 0.26243697,  0.24019621,  0.40906318],\n         [ 0.27956172,  0.25770321,  0.42649251]]]])>,) and kwargs: {} for signature: (*, input: TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name='input')) -> Dict[['output', TensorSpec(shape=(None, 3), dtype=tf.float32, name='output')]].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m img_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(fore_img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     72\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m preprocess(img_rgb)\n\u001b[1;32m---> 73\u001b[0m pred, conf \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m label \u001b[38;5;241m=\u001b[39m CLASS_NAMES[pred]\n\u001b[0;32m     75\u001b[0m display_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconf\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m, in \u001b[0;36mload_model.<locals>.predict\u001b[1;34m(input_img)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     22\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(input_img)\n\u001b[1;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs\n\u001b[0;32m     25\u001b[0m conf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1171\u001b[0m, in \u001b[0;36mConcreteFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Executes the wrapped function.\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m \n\u001b[0;32m   1124\u001b[0m \u001b[38;5;124;03m  ConcreteFunctions have two signatures:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;124;03m    TypeError: If the arguments do not match the function's signature.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1171\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1183\u001b[0m, in \u001b[0;36mConcreteFunction._call_impl\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m structured_err:\n\u001b[0;32m   1182\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_flat_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1184\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m flat_err:\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(  \u001b[38;5;66;03m# pylint: disable=raise-missing-from\u001b[39;00m\n\u001b[0;32m   1186\u001b[0m         \u001b[38;5;28mstr\u001b[39m(structured_err)\n\u001b[0;32m   1187\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFallback to flat signature also failed due to: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1188\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(flat_err)\n\u001b[0;32m   1189\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1243\u001b[0m, in \u001b[0;36mConcreteFunction._call_with_flat_signature\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1238\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1239\u001b[0m       arg, (tensor_lib\u001b[38;5;241m.\u001b[39mTensor, resource_variable_ops\u001b[38;5;241m.\u001b[39mBaseResourceVariable)):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_signature_summary()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: expected argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1241\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(zero-based) to be a Tensor; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1242\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:146\u001b[0m, in \u001b[0;36m_WrapperFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# cross-replica context\u001b[39;00m\n\u001b[0;32m    145\u001b[0m   captured_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(get_unused_handle, captured_inputs))\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: cannot compute __inference_signature_wrapper_2241 as input #0(zero-based) was expected to be a float tensor but is a double tensor [Op:__inference_signature_wrapper_2241]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "MODEL_DIR = \"trained_models\"\n",
    "CLASS_NAMES = ['black', 'brown', 'white']\n",
    "TF_SAVED_MODEL_PATH = os.path.join(MODEL_DIR, \"tf_saved_model\")\n",
    "\n",
    "def load_model():\n",
    "    import tensorflow as tf\n",
    "    model = tf.saved_model.load(TF_SAVED_MODEL_PATH)\n",
    "    infer = model.signatures[\"serving_default\"] if \"serving_default\" in model.signatures else model\n",
    "    def preprocess(img):\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "        img = img[np.newaxis, ...]\n",
    "        return img\n",
    "    def predict(input_img):\n",
    "        import tensorflow as tf\n",
    "        inputs = tf.convert_to_tensor(input_img)\n",
    "        outputs = infer(inputs)\n",
    "        logits = list(outputs.values())[0] if isinstance(outputs, dict) else outputs\n",
    "        conf = tf.nn.softmax(logits, axis=-1)\n",
    "        pred = tf.argmax(conf, axis=-1).numpy()[0]\n",
    "        return pred, float(conf[0][pred])\n",
    "    return preprocess, predict\n",
    "\n",
    "# The rest of the script (face detection, loop, etc.)copy from the PyTorch example above.\n",
    "preprocess, predict = load_model()\n",
    "frontal_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_profileface.xml\")\n",
    "def forehead_coords(face):\n",
    "    x, y, w, h = face\n",
    "    fw = int(w * 0.5)\n",
    "    fh = int(h * 0.20)\n",
    "    fx = x + (w - fw) // 2\n",
    "    fy = y + int(h * 0.10)\n",
    "    return fx, fy, fw, fh\n",
    "cap = cv2.VideoCapture(0)\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    elapsed = int(time.time() - start_time)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = frontal_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "    detected_type = \"Frontal\"\n",
    "    if len(faces) == 0:\n",
    "        faces = profile_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "        detected_type = \"Profile\"\n",
    "    if len(faces) == 0:\n",
    "        flipped = cv2.flip(gray, 1)\n",
    "        faces_flip = profile_cascade.detectMultiScale(flipped, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))\n",
    "        if len(faces_flip) > 0:\n",
    "            faces = []\n",
    "            for (x, y, w, h) in faces_flip:\n",
    "                x_new = gray.shape[1] - x - w\n",
    "                faces.append((x_new, y, w, h))\n",
    "            detected_type = \"Profile (Flipped)\"\n",
    "    if len(faces) == 0:\n",
    "        cv2.putText(frame, \"No face detected\", (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0, 0, 255), 2)\n",
    "    else:\n",
    "        faces = sorted(faces, key=lambda x: x[2]*x[3], reverse=True)\n",
    "        (x, y, w, h) = faces[0]\n",
    "        fx, fy, fw, fh = forehead_coords((x, y, w, h))\n",
    "        fore_img = frame[fy:fy+fh, fx:fx+fw]\n",
    "        if fore_img.shape[0] > 10 and fore_img.shape[1] > 10:\n",
    "            img_rgb = cv2.cvtColor(fore_img, cv2.COLOR_BGR2RGB)\n",
    "            input_tensor = preprocess(img_rgb)\n",
    "            pred, conf = predict(input_tensor)\n",
    "            label = CLASS_NAMES[pred]\n",
    "            display_text = f\"{label} ({conf * 100:.1f}%)\"\n",
    "        else:\n",
    "            display_text = \"N/A\"\n",
    "        overlay = frame.copy()\n",
    "        cv2.rectangle(overlay, (fx, fy), (fx+fw, fy+fh), (255, 220, 0), -1)\n",
    "        frame = cv2.addWeighted(overlay, 0.3, frame, 0.7, 0)\n",
    "        cv2.rectangle(frame, (fx, fy), (fx+fw, fy+fh), (255, 220, 0), 2)\n",
    "        cv2.putText(frame, display_text, (fx, fy-5), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,220,0), 2)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (128,255,0), 1)\n",
    "        cv2.putText(frame, f\"{detected_type}\", (x, y-25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,128,255), 2)\n",
    "    cv2.putText(frame, f\"{elapsed} seconds\", (10, frame.shape[0]-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    cv2.imshow(\"Skin Tone Analysis (forehead/side profile)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3347d3-958c-46d0-b1ca-274b34218297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf15)",
   "language": "python",
   "name": "tf15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
